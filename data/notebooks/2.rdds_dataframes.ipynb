{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff4b9dd",
   "metadata": {},
   "source": [
    "## Connect to Spark standalone cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbe2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n",
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark--org.apache.spark.deploy.master.Master-1-sparkc.out\n",
      "localhost: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-sparkc.out\n",
      "162 NameNode\n",
      "867 NodeManager\n",
      "5027 Master\n",
      "740 ResourceManager\n",
      "502 SecondaryNameNode\n",
      "311 DataNode\n",
      "5131 Worker\n",
      "5212 Jps\n",
      "3948 SparkSubmit\n",
      "no sc/spark to stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/29 03:30:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/29 03:30:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "%run 0.connect-to-spark-cluster.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd16876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sparkc:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sparkc:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LearningSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa01810bfa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7cd5e",
   "metadata": {},
   "source": [
    "## RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50c9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "data_rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb69b736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d2333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbl_distData = data_rdd.map(lambda x: x*2)\n",
    "dbl_distData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1778b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53fc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rows = [\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "]\n",
    "\n",
    "rows_rdd = sc.parallelize(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35226704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3264ae3-3ba2-40ba-a035-d4e56b80f97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc83c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95058a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_rdd = sc.parallelize(range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0720f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(range_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_range_rdd = range_rdd.map(lambda x: x*3)\n",
    "cube_range_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f69dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rows_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0ffb8",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7952f5",
   "metadata": {},
   "source": [
    "### Infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a79db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows)\n",
    "print(type(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb931e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows_rdd)\n",
    "print(type(rows_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_df = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_rdd_df = spark.createDataFrame(rows_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def553fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows_df)\n",
    "print(type(rows_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35412ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows_rdd_df)\n",
    "print(type(rows_rdd_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a068e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5896319",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark.createDataFrame(data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01541927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267f833d",
   "metadata": {},
   "source": [
    "### Explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a64b610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24340599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7ffea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | string2             \n",
      " d   | 2000-02-01          \n",
      " e   | 2000-01-02 12:00:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39456def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e1f4a1",
   "metadata": {},
   "source": [
    "### examine dataframe api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d00b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec95472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca678ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "914e3e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[a#0L,b#1,c#2,d#3,e#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a19f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('a', LongType(), True), StructField('b', DoubleType(), True), StructField('c', StringType(), True), StructField('d', DateType(), True), StructField('e', TimestampType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c77fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf50c523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, a: string, b: string, c: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c8ad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced8dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1aa394a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o155.toDF.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\n\tat org.apache.spark.sql.Dataset.$anonfun$toDF$3(Dataset.scala:487)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.Dataset.toDF(Dataset.scala:487)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m textFile \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile:///data/flight-data/csv/2010-summary.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:3148\u001b[0m, in \u001b[0;36mDataFrame.toDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3136\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` that with new specified column names\u001b[39;00m\n\u001b[1;32m   3137\u001b[0m \n\u001b[1;32m   3138\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;124;03m    [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3148\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o155.toDF.\n: java.lang.ClassCastException: class java.util.ArrayList cannot be cast to class java.lang.String (java.util.ArrayList and java.lang.String are in module java.base of loader 'bootstrap')\n\tat org.apache.spark.sql.Dataset.$anonfun$toDF$3(Dataset.scala:487)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.Dataset.toDF(Dataset.scala:487)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.read.text(\"file:///data/flight-data/csv/2010-summary.csv\").toDF(['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0534c810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f0de4e8-1d23-40a2-a39f-5d622414fed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m textdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile:///data/flight-data/csv/2010-summary.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 517\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:519\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    517\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    518\u001b[0m     _merge_type,\n\u001b[0;32m--> 519\u001b[0m     (\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data),\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/types.py:1302\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1304\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types\n",
    "textdf = spark.createDataFrame(\"file:///data/flight-data/csv/2010-summary.csv\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9be3fb60-de20-48b2-bda5-f08f9db0c19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"value\":\"PAR1\\\\u0015\\\\u0004\\\\u0015�\\\\u001A\\\\u0015�\\\\u000EL\\\\u0015�\\\\u0001\\\\u0015\\\\u0004\\\\u0000\\\\u0000\\\\u001F�\\\\b\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0003UUMs\\\\u001B7\\\\f�!V�|�ۤ��:�=\\x7fBVdY��Q,ۇް��.\\\\\".��VV��\\\\u0000\\\\u0018�Ӌ�%A���\\\\u0001zwvv��(be�\\\\u0011\\\\\"��|0��.�.Ə\\\\u0004�\\\\u0007\\\\u0002k�\\\\u001C�k>��>���\\\\u0012~�-:���\\u061c�\\\\u0001\\\\u001CH�\\\\u0015ب�W��3�\\\\u0005�\\\\u0013W\\'�\\\\u0016�b�>�]o�Uf\"}',\n",
       " '{\"value\":\"T��,z�_���@.�Gr%�\\x7f�\"}',\n",
       " '{\"value\":\"�y@\\\\u0007\\\\u0015CP��\\\\bv��\\\\u001B�\\\\u0011�q�kQ�\\\\u0004����o��c\\\\\\\\�}�~b[�\\\\u0014jyx�]�\\\\u0002�o\\\\u0004\"}',\n",
       " '{\"value\":\"ǿ�\\\\u0006��9�f\\\\u000B�\\\\u0019*\\\\u001F�\\\\u0015O��%\\\\u001C`�H6���)\\\\u001E0H���)�W\\\\u0000!�\\\\u001B���Y�\\\\u000F� ��\"}',\n",
       " '{\"value\":\"*���\\\\u0017P{��\\\\u0016�/n��?s%�N�\"}',\n",
       " '{\"value\":\"-��\"}',\n",
       " '{\"value\":\"\\x7f2=RܬL\\\\u0002�B���-\\\\b�a��\\\\u001E���\\\\u0006H�H�G��!1\\\\u001B#���i3V>���\\\\u001F�\\\\u0019��\\\\u0004\"}',\n",
       " '{\"value\":\"�\"}',\n",
       " '{\"value\":\"B�*-q�\\\\u0015\\\\u0015@�s�kܛ\\x7f\\\\u0011$�<��\\\\u0002Y\"}',\n",
       " '{\"value\":\"b�$WC\\\\u0018F�\\\\u0003V��Z�\\\\u0001�\"}',\n",
       " '{\"value\":\"�Yl�w�!�\\\\u0015�������\\\\\\\\��=��*r�ϵ�\\\\u0012x��\\\\u00057(�C��é�A�{��\\\\u0005C��\\\\u000E]H*�o\\\\u0010!�b㚾ӟ�TK�1+m���˛{�=8ys\\\\u0003\\\\u0014I��iϦ\\\\u0006d�*�[�p]\\\\u0001^\\\\u0006��\\'��2x�`f�+��S�\\\\u0007�_|�E���\\\\u001Dv��T��C��_�\\\\u0013�r6\\\\t��wrƊ.\\\\u001B��_C�p\\\\u0015�\\'!$0���\\\\u0019\\\\b5���Gz\\\\u0003\\\\u0007��/�7�\\\\u000E����\\\\u001CO\\\\u0002\\\\u0014f֒���Y�5t>`n*S��Uڦ\"}',\n",
       " '{\"value\":\"�M ~A�B�\\\\t�\\\\u001B\\\\fi�Pk\\\\u001EA�\\x7f��\\\\fܙ#\\\\u0018��O�1���u�t�����0\\\\\"4kQ�{��e�I\\\\u000F�\\\\u0014��H\\\\u001A\\\\u0007RE��r�\\\\u0003reG\\\\u001E�C[Sj\\\\u0005��n��f~�3\\\\\\\\����L\\\\u0017�Ӧ!K]\\'�Botw��\\\\u0005\\\\u001D�G\\\\f\\\\u0015���.0d����3�c�b�[�\\\\u001A{s\\\\u0012�m*�ln \\\\u0000��\"}',\n",
       " '{\"value\":\"_v\\\\u0018\\\\u000E�_x_2͕o_)�B���з>�ěR\\\\u000B�F2��H\\\\u0018y@G�L:�`X�\\\\u001B�$\\\\u001E��ǐ��Շ*Kw�\\\\u0000�\\\\u001C=��sƪ�Xr�\\u0557���ژ93��si\\\\u0015Z�:-�:\\\\u0000�X]\\\\u0011�8�k�e�Ǚ5<��eb\\\\u0015��\\\\u0001�\\\\u0007�!$a�.%�;�C�:Y�\\\\u0010\\\\u0004�菀\\\\u0003\"}',\n",
       " '{\"value\":\"��蒜q\\\\u0004�|�?�Xn�P�����\\\\u0000�\"}',\n",
       " '{\"value\":\"�\\\\u0013�\\\\u0006\\\\u0000\\\\u0000\\\\u0015\\\\u0000\\\\u0015�\\\\u0003\\\\u0015�\\\\u0003,\\\\u0015�\\\\u0003\\\\u0015\\\\u0004\\\\u0015\\\\u0006\\\\u0015\\\\b\\\\u001C\\\\u0018\\\\u0007Vietnam\\\\u0018\\\\u000BAfghanistan\\\\u0016\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u001F�\\\\b\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0003cf``����������������`� �p��!�A��]E�чa\"}',\n",
       " '{\"value\":\"ׁ��)\\\\f�x�/0\\\\u0002�7�30�0,\\\\u0010\\\\u00002;\\\\u0018\\\\u001A\\\\u0004\\\\u001BB�f�1�0L`b`�bH`M��� �𡉑����\"}',\n",
       " '{\"value\":\"���\\\\u0000\\\\u000FÂ�9�1~ۙ?0��3|`\\\\u0006�)8X�\\\\b0�>\\\\u0016l`\\\\u0001*q\\\\b�d`�vf�0C��b �ji\\\\u0018#C�h\\\\u0001�\\\\u001D�o�q3��7``��fl\\\\u0000\\\\u0019:�1�$�\\\\u00033�p\\\\u0006\\\\u000F � �!m;�;s\\\\u001B=�3/\\\\u001Bvɂ��Aă�\\\\f�\\\\f\\\\f7>3\\\\u0002\\\\u0000 �\\\\u0005+�\\\\u0000\\\\u0000\\\\u0000\\\\u0015\\\\u0004\\\\u0015�\\\\u001B\\\\u0015�\\\\u000FL\\\\u0015�\\\\u0002\\\\u0015\\\\u0004\\\\u0000\\\\u0000\\\\u001F�\\\\b\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0003UT�W\\\\u001B7\\\\u0010�\\\\u0000\\\\u0004H��kӾ���{�\\\\t۱�\\\\u0018�(k8�6�;�N����\\\\u0012����h�Ms\\\\u0001��h��~̻���\\\\u0007߁#x\\'�\\\\u001BF\\\\u000B�>IGW\\\\u0013|�ã���)\\\\u0002\\\\u0004\\\\u001C�勂\\\\\\\\\\\\u0003�gL\\\\u0015W�\\\\u000Ej���\\\\u001D�Ђ��fH�\\\\f\\\\u001F��`�\\\\u00008�{/�\\'�M+͆\\\\u0000�4���@�YN\\\\u000B��᭑���*�\\\\u000E�F*]ah���Ը@�\"}',\n",
       " '{\"value\":\"����5��Y>E�Vg�W\\\\u0011j���l�9\\\\u000E\\x7f�i�\\\\u0007\\\\u0019��#�\\\\u001Ay��\\x7f!����C�\\\\u0007/�}/\\\\u0000�\\\\u0003U�潊\\\\u0002��ܡ�,\\\\u0005\\\\u001D�5��<!���7�@*Δ\\\\t\\\\u000B�)����\\'J���\\\\u000E\\\\u0005t^\\x7f��\\\\u0006��˓̳���/��u��d�43�\\\\u0015&&�-�)��O����1K��z��\\\\u0013O�A���e�\\\\u0002�_���w�\\\\u0004�3\\\\u000F���R�j��؇4�m��]�#7��H�p�x��SQ�E�ݘ~]G��D\\\\u000F�\\\\u000B���\\\\u000E�\\\\u0004�\\\\u000E*v�ch\\\\u0005\\\\u0019�\"}',\n",
       " '{\"value\":\"8��\\\\u0000��DIe�Ql��V!U\\\\u0004�\\\\u0013\\\\u0004�r8\\\\u000F-�� \\\\u0001�p,��u\\\\u000B�@}��\\\\u000Ff�Ȏß��\\\\u0006Ux&z�aoG\\\\u0015\\\\u00196꽳L[\\\\u001D\\\\u0019�L�T&\\\\u001DN�v��a�\\'r\\\\u0015�����3��$���֨��ث\\\\u0003\\\\u001E7,5*ւA��i�y\\\\u000Bcj�$�\\\\u0012\\\\u0002il�%tc������0��w�CP++[ɄK�����\\\\\"G,���F����\\\\tCi�\\\\u001D����h�ٜ�{�\\\\u000F�}\\\\u0002���\\\\u0012�\\\\u0019��\\\\u0016��m(v\\\\u001A�J�\\x7f�d72�&q\\\\b���\\\\\"��_�\\\\\"\\\\u0007���h��rGƊ�\\\\u0017e��R��\\\\u0019\\\\u0018�j\\\\u001Fh\\\\u00131�\\\\u000F��Ɗ@��\\\\u0014hh�en��R��\\\\u00112���:�ͥ��G0���(\\\\u0013�n\\\\u0018�5)\\\\u0007\\\\u0019�L4�@\\\\t\\\\\\\\\\\\u0003m�}88q���HK�\\\\u000E\\\\t+z\\\\u0001�+/�UI�-�]�։&���jʰ#{�� �_�$X�J���MH�\\\\u001D�j�鴛�ҼƎ5��Iw\\\\u0012���B,U�F�6��t�\\\\u0005\\\\u0012��P�]\\\\u0019k\\\\u001Dv*�\\\\u0016e�\\\\u0002\\\\u0006��\\\\u0016�\\\\u000Bi2��N�f;���\\\\t?��\\\\b��������b&�}6�r�fܥ���9\"}',\n",
       " '{\"value\":\"�1{�;\\\\u0001q�\\\\f�0μ�]�S�7\\\\u0004`��������<��\\\\u0014\\\\u0011�b�}�4�Z�(�P�k_B㵍��:Qa���\\\\u0015~?�fI!�d�����o܉\\\\u001D��\\\\u0004�Z6�����E�\\\\u000F^�4��\\\\u0006\\\\u0000\\\\u0000\\\\u0015\\\\u0000\\\\u0015�\\\\u0004\\\\u0015�\\\\u0003,\\\\u0015�\\\\u0003\\\\u0015\\\\u0004\\\\u0015\\\\u0006\\\\u0015\\\\b\\\\u001C\\\\u0018\\\\u0007Vietnam\\\\u0018\\\\u000BAfghanistan\\\\u0016\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u001F�\\\\b\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0003-�\\\\t3\\\\u0002P\\\\u0014�_>K�lQ�d�J*����\\\\u001A���H�l��\\\\u000F7�k|3�̹g��E\\\\b�F�\\\\u0013�<�/�B��h��\\\\u0004\\\\u001D9J)�r��R\\\\u0015TVU�\\\\u0018��3b�zsC#M�X�\\\\u0005Zik���\\\\u000B��B�\"}',\n",
       " '{\"value\":\"�\\\\u0003z���p{�\\\\u0007\\\\u0006�\\\\u0018\\\\u001E�Q�`|�I�\\\\u0014�`�\\\\u0019��27���\\\\u0004�+�j��.mcsK��\\\\u001D?���������`\\\\b�G\\\\u001CG�\\\\u0012��8����E꒫�[���>�/\"}',\n",
       " '{\"value\":\"\\\\u000F����������\\\\u0013�|}���\\\\u0003���\\\\u0015\\\\t\\\\u0001\\\\u0000\\\\u0000\\\\u0015\\\\u0004\\\\u0015�\\\\u0013\\\\u0015�\\\\u0006L\\\\u0015�\\\\u0002\\\\u0015\\\\u0004\\\\u0000\\\\u0000\\\\u001F�\\\\b\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u00035�?H�q\\\\u0018\\\\u0007�W�˰�.3-�Σ���bH��A��y�\\\\u0019\\\\u0015�$x�\\\\u0004\\\\t�\\\\\\\\��\\\\u0012��[\\\\u001A\\\\u0002�\\\\\\\\\\\\\\\\,\\\\u0014��\\\\u001C\\\\u001D\\\\u000B\\\\u001A\\\\\\\\� \\\\u0007\\\\u0007�E$����s�|�>��{�\\\\u001F�˕\\\\u0005᯲,��\\\\\\\\�S�ɢ��r3clg\\\\u000Bk9�rN��^��üepTN�\\\\t���>\\\\u0010z��\\\\u0019���\\\\u000En�?!w�\\\\u001F��\\\\b\\\\u001F���\\\\\"��3�=9ţ�1\\\\u000F�s��z�\\\\\\\\���\\\\u0007����_��/�;&���ʅ��\\\\u0011�{�ྏ�:>�$_�c�s��f�~5�Գl�[�Y��<�\\\\u001A�\\\\u001D�4��Т�f�?�>M�*���r�>�A�7Xp~ӹn����^���������T~�e��*\\x7fs�s�/}o��\\'x���_�yV�\\\\u001A���5��灜��~V�U�(����\\\\u001A�O��G�?���㡟d18�\"}',\n",
       " '{\"value\":\"��\\\\u001C\\\\u0017�m�/3�;u��\\\\\\\\�6�\\\\u001B�\\\\u0013��W^�%��\\\\u0017�\\\\u0003�����\\\\u0004\\\\u0000\\\\u0000\\\\u0015\\\\u0000\\\\u0015�\\\\u0004\\\\u0015�\\\\u0004,\\\\u0015�\\\\u0003\\\\u0015\\\\u0004\\\\u0015\\\\u0006\\\\u0015\\\\b\\\\u001C\\\\u0018\\\\b�O\\\\u0005\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0018\\\\b\\\\u0001\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0016\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u001F�\\\\b\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0003\\\\u0005�\\\\u0007?\\\\u0002a\\\\u0000\\\\u0007�\\x7f�N�+d��ʙ�+Y\\\\t\\\\u0019q�\\\\u001Eٙ�;{Ϭ��?���a\\\\u0000�1*�\\\\u001B�0\\\\u0006,\\\\u0017��hu�\\\\u001B\\\\\"\\\\b\\\\\"�Q�1�ظ��D=/��D�5YJ���4�gd�YB�YMi��G�]̣�|\\\\u0014\\\\u0014��Q\\\\f\\'[\\x96��$K9���\\\\u0004����F\\\\u0001j�:���(�M��ֶv=:Hg\\\\u0017�f�=��>�\\x7f`ph�;\\x8e��C4��Iߔ}zf���_X\\\\\\\\Z\\\\u0016�vŰ�\\\\u0006���H��-��\"}',\n",
       " '{\"value\":\"ng��rX��}rpxt̟����qa��\\\\b\\\\\\\\z����on=R�����\\\\u0010z��(C�_^�B�\\\\u001F�ϯop�\\\\u000F�\\\\u000B\\\\u001A�?�\\\\u000Fʙ\\\\t\\\\u0001\\\\u0000\\\\u0000\\\\u0015\\\\u0002\\\\u0019LH\\\\fspark_schema\\\\u0015\\\\u0006\\\\u0000\\\\u0015\\\\f%\\\\u0002\\\\u0018\\\\u0011DEST_COUNTRY_NAME%\\\\u0000\\\\u0000\\\\u0015\\\\f%\\\\u0002\\\\u0018\\\\u0013ORIGIN_COUNTRY_NAME%\\\\u0000\\\\u0000\\\\u0015\\\\u0004%\\\\u0002\\\\u0018\\\\u0005count\\\\u0000\\\\u0016�\\\\u0003\\\\u0019\\\\u001C\\\\u0019<&\\\\b\\\\u001C\\\\u0015\\\\f\\\\u00195\\\\u0004\\\\u0006\\\\b\\\\u0019\\\\u0018\\\\u0011DEST_COUNTRY_NAME\\\\u0015\\\\u0004\\\\u0016�\\\\u0003\\\\u0016�\\\\u001E\\\\u0016�\\\\u0013&\\\\b<\\\\u0018\\\\u0007Vietnam\\\\u0018\\\\u000BAfghanistan\\\\u0016\\\\u0000\\\\u0000\\\\u0000\\\\u0000&�\\\\u0013\\\\u001C\\\\u0015\\\\f\\\\u00195\\\\u0004\\\\u0006\\\\b\\\\u0019\\\\u0018\\\\u0013ORIGIN_COUNTRY_NAME\\\\u0015\\\\u0004\\\\u0016�\\\\u0003\\\\u0016� \\\\u0016�\\\\u0013&�\\\\u0013<\\\\u0018\\\\u0007Vietnam\\\\u0018\\\\u000BAfghanistan\\\\u0016\\\\u0000\\\\u0000\\\\u0000\\\\u0000&�\\'\\\\u001C\\\\u0015\\\\u0004\\\\u00195\\\\u0004\\\\u0006\\\\b\\\\u0019\\\\u0018\\\\u0005count\\\\u0015\\\\u0004\\\\u0016�\\\\u0003\\\\u0016�\\\\u0018\\\\u0016�\\\\u000B&�\\'<\\\\u0018\\\\b�O\\\\u0005\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0018\\\\b\\\\u0001\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0016\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0016�X\\\\u0016�\\\\u0003\\\\u0000\\\\u0019\\\\u001C\\\\u0018)org.apache.spark.sql.parquet.row.metadata\\\\u0018�\\\\u0001{\\\\\"type\\\\\":\\\\\"struct\\\\\",\\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"DEST_COUNTRY_NAME\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\",\\\\\"nullable\\\\\":true,\\\\\"metadata\\\\\":{}},{\\\\\"name\\\\\":\\\\\"ORIGIN_COUNTRY_NAME\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\",\\\\\"nullable\\\\\":true,\\\\\"metadata\\\\\":{}},{\\\\\"name\\\\\":\\\\\"count\\\\\",\\\\\"type\\\\\":\\\\\"long\\\\\",\\\\\"nullable\\\\\":true,\\\\\"metadata\\\\\":{}}]}\\\\u0000\\\\u0018;parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)\\\\u0000�\\\\u0002\\\\u0000\\\\u0000PAR1\"}']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7292712",
   "metadata": {},
   "source": [
    "### examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2caf0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46176191",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/flight-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a77321",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/flight-data/csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /data/flight-data/csv/2010-summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /data/flight-data/csv/2010-summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158431a4",
   "metadata": {},
   "source": [
    "### create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22132232",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDF = spark.read.option(\"header\", \"true\").csv(\"file:///data/flight-data/csv/2010-summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
