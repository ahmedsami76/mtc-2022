{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b928812-b656-43ea-9bc9-5ea0aeb6f1cf",
   "metadata": {},
   "source": [
    "## Connect to Spark standalone cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ee9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/spark/sbin/stop-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de07cc8-3ca8-494f-9850-735c7f80a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/spark/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315b3ae-2ffd-4831-ba1f-2591e0b1f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b2d24-1af4-432d-8040-270403530b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf, pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    print(\"No Spark Session\")\n",
    "\n",
    "sparkClassPath =  '/usr/local/spark/jars/sqljdbc42.jar'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.driver.extraClassPath\", sparkClassPath) \\\n",
    "        .config(\"spark.jars\", sparkClassPath) \\\n",
    "        .appName(\"Southbridge Analytics\") \\\n",
    "        .master(\"spark://sparkc:7077\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8c61d-1f05-4d7b-a331-a17f3a3a7c72",
   "metadata": {},
   "source": [
    "## Connect to SQL Server and create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265449d-49d7-4d07-a643-889fc01c9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"sql\"\n",
    "database = \"adworks\"\n",
    "table = \"SalesLT.Customer\"\n",
    "user = \"sa\"\n",
    "password  = \"P@ssw0rd\"\n",
    " \n",
    "\n",
    "sql1 = \"\"\"\n",
    "SELECT c.CustomerID, c.CompanyName,COUNT(soh.SalesOrderID) AS OrderCount \n",
    "FROM SalesLT.Customer AS c LEFT OUTER JOIN SalesLT.SalesOrderHeader AS soh \n",
    "ON c.CustomerID = soh.CustomerID \n",
    "GROUP BY c.CustomerID, c.CompanyName\n",
    "\"\"\"\n",
    "\n",
    "sql2 = \"\"\"\n",
    "SELECT c.CompanyName, a.AddressLine1, ISNULL(a.AddressLine2, '') AS AddressLine2,\n",
    "a.City, a.StateProvince, a.PostalCode, a.CountryRegion, oh.SalesOrderID, oh.TotalDue\n",
    "FROM SalesLT.Customer AS c\n",
    "JOIN SalesLT.SalesOrderHeader AS oh\n",
    "ON oh.CustomerID = c.CustomerID\n",
    "JOIN SalesLT.CustomerAddress AS ca\n",
    "ON c.CustomerID = ca.CustomerID AND AddressType = 'Main Office'\n",
    "JOIN SalesLT.Address AS a\n",
    "ON ca.AddressID = a.AddressID\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#read table data into a spark dataframe\n",
    "jdbcDF1 = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n",
    "    .option(\"query\", sql1) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n",
    "    .option(\"query\", sql2) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346217b-556f-43de-93b1-5e7f2980eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF1.show(5)\n",
    "jdbcDF2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435859cb-9f7b-4ea8-99ea-be04792aabb3",
   "metadata": {},
   "source": [
    "## Write DataFrame to HDFS as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6ee95-6e18-47a2-8063-a777e8d9fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir in hdfs if not already there\n",
    "os.system('hdfs dfs -mkdir hdfs://localhost:9000/sql-spoke/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d1918-8c5e-494a-a6bd-f41fb2c23bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF1.write.parquet(\"hdfs://localhost:9000/sql-spoke/sql1.parquet\")\n",
    "jdbcDF2.write.parquet(\"hdfs://localhost:9000/sql-spoke/sql2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1e4be-d77e-4bce-8d2d-464b456d21b3",
   "metadata": {},
   "source": [
    "## Validate Parquet from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fc615-a787-4fed-b753-5b2f81655c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf1 = spark.read.parquet(\"hdfs://localhost:9000/sql-spoke/sql1.parquet\")\n",
    "testdf2 = spark.read.parquet(\"hdfs://localhost:9000/sql-spoke/sql2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ee988-1886-46b2-863d-2f6acddcd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf1.show(2)\n",
    "testdf2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749ef18-3256-45c3-b604-68d2803c0aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
